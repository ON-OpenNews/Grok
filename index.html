<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <title>Parle à Grok</title>
    <style>
        body { font-family: Arial, sans-serif; text-align: center; margin-top: 20px; background-color: #f0f0f0; }
        button { padding: 10px 20px; font-size: 16px; cursor: pointer; margin: 5px; background-color: #1DA1F2; color: white; border: none; border-radius: 5px; }
        button:disabled { background-color: #cccccc; }
        select { padding: 10px; font-size: 16px; margin: 5px; }
        textarea { width: 80%; height: 400px; margin-top: 20px; padding: 10px; font-size: 16px; border: 1px solid #ddd; border-radius: 5px; resize: vertical; }
    </style>
</head>
<body>
    <h1>Parle-moi !</h1>
    <button id="start-btn">Démarrer le micro</button>
    <button id="stop-btn" disabled>Arrêter le micro</button>
    <button id="read-all-btn">Lire tout le texte</button>
    <select id="voice-select">
        <option value="DGTOOUoGpoP6UZ9uSWfA">Homme (Adam)</option>
        <option value="LPsbEdCpBjNTSJwvdME7">Femme (Rachel)</option>
        <option value="Paul">Paul (Native PC)</option>
    </select>
    <br>
    <textarea id="output" placeholder="Ton texte apparaîtra ici..."></textarea>

    <script>
        const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
        recognition.lang = 'fr-FR';
        recognition.continuous = true;
        recognition.interimResults = true;

        const startBtn = document.getElementById('start-btn');
        const stopBtn = document.getElementById('stop-btn');
        const readAllBtn = document.getElementById('read-all-btn');
        const voiceSelect = document.getElementById('voice-select');
        const output = document.getElementById('output');

        let silenceTimer;
        let isWaiting = false;
        let lastSpeechTime = Date.now();
        let currentText = '';
        let isSpeaking = false;
        let lastResponse = '';
        let recognitionActive = false;

        // Clés API intégrées
        const XAI_API_KEY = 'xai-YClC2sjxeGgsiOORtTWBaJakDX09lvv3MUq1sYGA4uOSDEWqfHbe3XKbWCU2XnMmIbe9NTrOrZhnUkzl';
        const ELEVENLABS_API_KEY = 'sk_9fd8d60e396f354f9cc3b46b28cf28aeb42cc981d9df6e11';

        startBtn.addEventListener('click', () => {
            console.log("Clic sur Démarrer le micro");
            if (!recognitionActive) {
                try {
                    recognition.start();
                    recognitionActive = true;
                    startBtn.textContent = 'Micro activé (parle !)';
                    startBtn.disabled = true;
                    stopBtn.disabled = false;
                } catch (error) {
                    console.log("Erreur au démarrage : ", error);
                }
            }
        });

        stopBtn.addEventListener('click', () => {
            console.log("Clic sur Arrêter le micro");
            if (recognitionActive) {
                recognition.stop();
                recognitionActive = false;
            }
            startBtn.textContent = 'Démarrer le micro';
            startBtn.disabled = false;
            stopBtn.disabled = true;
        });

        recognition.onresult = (event) => {
            const result = event.results[event.results.length - 1];
            let texte = result[0].transcript;
            texte = texte.replace(/virgule/gi, ',').replace(/point/gi, '.').replace(/question/gi, '?');

            const displayText = texte.trim();

            if (result.isFinal && !isSpeaking && displayText !== lastResponse) {
                currentText = displayText;
                output.value += (output.value ? '\n' : '') + currentText;
                console.log("Texte capté : ", displayText);

                if (texte.toLowerCase().includes("attends attends")) {
                    isWaiting = true;
                    setTimeout(() => { isWaiting = false; }, 5000);
                    return;
                }

                lastSpeechTime = Date.now();
                clearTimeout(silenceTimer);

                silenceTimer = setTimeout(() => {
                    if (!isWaiting) {
                        handleSpeechResponse(texte);
                    }
                }, 2000);
            }
        };

        async function handleSpeechResponse(userInput) {
            isSpeaking = true;
            if (recognitionActive) {
                recognition.stop();
                recognitionActive = false;
            }
            const reponse = await getGrokResponse(userInput);
            lastResponse = reponse;
            output.value += '\n' + reponse;
            console.log("Réponse affichée : ", reponse);
            await speakText(reponse);
            isSpeaking = false;
        }

        // Appel API xAI
        async function getGrokResponse(userInput) {
            const input = userInput.trim();
            console.log("Input reçu : ", input);
            try {
                const response = await fetch('https://api.x.ai/v1/chat/completions', {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${XAI_API_KEY}`,
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({
                        model: 'grok-beta',
                        messages: [
                            { role: 'system', content: 'Tu es Grok, un collègue fun et utile qui bosse avec moi sur des projets.' },
                            { role: 'user', content: input }
                        ],
                        temperature: 0.7
                    })
                });
                const data = await response.json();
                return data.choices[0].message.content;
            } catch (error) {
                console.log("Erreur API xAI : ", error);
                return "Oups, souci avec l’API xAI. On vérifie ça ?";
            }
        }

        // Lecture avec ElevenLabs ou Paul
        async function speakText(text) {
            const selectedVoice = voiceSelect.value;
            if (selectedVoice === 'Paul') {
                const utterance = new SpeechSynthesisUtterance(text);
                utterance.lang = 'fr-FR';
                utterance.voice = speechSynthesis.getVoices().find(voice => voice.name === 'Paul') || null;
                speechSynthesis.speak(utterance);
                console.log("Lecture avec Paul");
            } else {
                try {
                    const response = await fetch(`https://api.elevenlabs.io/v1/text-to-speech/${selectedVoice}`, {
                        method: 'POST',
                        headers: {
                            'Accept': 'audio/mpeg',
                            'Content-Type': 'application/json',
                            'xi-api-key': ELEVENLABS_API_KEY
                        },
                        body: JSON.stringify({
                            text: text,
                            model_id: 'eleven_multilingual_v2',
                            voice_settings: { stability: 0.5, similarity_boost: 0.75 }
                        })
                    });
                    if (response.ok) {
                        const audioBlob = await response.blob();
                        const audioUrl = URL.createObjectURL(audioBlob);
                        const audio = new Audio(audioUrl);
                        audio.play();
                        console.log("Lecture ElevenLabs OK avec voix : ", selectedVoice);
                    } else {
                        throw new Error("ElevenLabs KO");
                    }
                } catch (error) {
                    console.log("Erreur ElevenLabs, passage à Paul : ", error);
                    const utterance = new SpeechSynthesisUtterance(text);
                    utterance.lang = 'fr-FR';
                    utterance.voice = speechSynthesis.getVoices().find(voice => voice.name === 'Paul') || null;
                    speechSynthesis.speak(utterance);
                    console.log("Lecture avec Paul (backup)");
                }
            }
        }

        // Bouton "Lire tout le texte"
        readAllBtn.addEventListener('click', () => {
            const fullText = output.value.trim();
            if (fullText) {
                console.log("Clic sur Lire tout le texte");
                speakText(fullText);
            }
        });

        recognition.onend = () => {
            recognitionActive = false;
            startBtn.textContent = 'Démarrer le micro';
            startBtn.disabled = false;
            stopBtn.disabled = true;
            console.log("Reco terminée");
        };

        recognition.onerror = (event) => {
            console.log("Erreur reco : ", event.error);
            output.value += "\nErreur : " + event.error;
            recognitionActive = false;
            startBtn.textContent = 'Démarrer le micro';
            startBtn.disabled = false;
            stopBtn.disabled = true;
        };
    </script>
</body>
</html>
